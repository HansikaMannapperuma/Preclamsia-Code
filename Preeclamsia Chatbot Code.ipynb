{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf87a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and Split PDF Documents\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9258fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document\n",
    "loader = PyPDFLoader(\"m-1.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913af0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter for chunking\n",
    "define_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1400,  # Size of each chunk\n",
    "    chunk_overlap=180,  # Overlap between chunks\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5daf9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks from page 1: 1\n"
     ]
    }
   ],
   "source": [
    "# Example: Split text from a specific page\n",
    "chunks = define_text_splitter.split_text(pages[1].page_content)\n",
    "\n",
    "# Check the number of chunks\n",
    "print(f\"Total chunks from page 1: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdca2b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction\n",
      "Among pregnancy-related complications, hypertensive disorders of pregnancy signifi-\n",
      "cantly contribute to maternal and perinatal mortality on a global scale. In Latin America\n",
      "and the Caribbean, hypertensive disorders are responsible for almost 26% of maternal\n",
      "deaths, whereas in Africa and Asia they contribute to 9% of deaths [1].\n",
      "Preeclampsia (PE) is one of several hypertensive disorders of pregnancy and is defined\n",
      "by the International Society for the Study of Hypertension in Pregnancy (ISSHP) as gesta-\n",
      "tional hypertension after 20 weeksâ€™ gestation, accompanied by one or more of the following\n",
      "Int. J. Mol. Sci. 2024 ,25, 4532. https://doi.org/10.3390/ijms25084532 https://www.mdpi.com/journal/ijms\n",
      "Length: 719\n"
     ]
    }
   ],
   "source": [
    "# Print each chunk and its length\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(f\"Length: {len(chunk)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619558fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Pinecone and Store Embeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\",\n",
    "        encoding_format=\"float\",\n",
    "        dimensions=1536\n",
    "    )\n",
    "    return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=\"xxxxxxxxx\")\n",
    "spec = ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "index_name = 'preeclamsia'\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if 'preeclamsia' not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name='preclamsia',\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "indexname = pc.Index(index_name)\n",
    "print(indexname.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040db306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert chunks into Pinecone\n",
    "for item in chunks:\n",
    "    chunk_id = item[\"chunk_id\"]\n",
    "    content = item[\"content\"]\n",
    "    embedding = get_embedding(content)\n",
    "    indexname.upsert(\n",
    "        vectors=[{\n",
    "            \"id\": chunk_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\"content\": content}\n",
    "        }]\n",
    "    )\n",
    "    print(f\"Inserted chunk ID: {chunk_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Query Pinecone and GPT\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "OPENAI_API_KEY = \"xxxxxxxxx\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ebe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone(query):\n",
    "    \"\"\"Query Pinecone using the embedding of the user query.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    results = indexname.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=3,\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f625fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preeclampsia_info(query):\n",
    "    \"\"\"Fetch preeclampsia information using Pinecone and GPT-3.5.\"\"\"\n",
    "    pinecone_results = query_pinecone(query)\n",
    "\n",
    "    if not pinecone_results['matches']:\n",
    "        return \"I couldn't find relevant information in the knowledge base.\"\n",
    "\n",
    "    top_chunks = [match['metadata']['content'] for match in pinecone_results['matches']]\n",
    "    top_chunks_content = \"\\n\".join(top_chunks)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a preeclampsia chatbot. Answer the user's question using the following knowledge:\"},\n",
    "            {\"role\": \"system\", \"content\": top_chunks_content},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78437c68",
   "metadata": {},
   "source": [
    "# Chatbot Interaction- stuffing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ada51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop to interact with the user\n",
    "print(\"Hi!!! I am Clamsy. Ask me questions about PREECLAMSIA/n\")\n",
    "user_query = \"\"\n",
    "while True:\n",
    "    user_query = input(\"\")\n",
    "    if user_query.lower() == \"quit\":\n",
    "        break\n",
    "    \n",
    "    info = get_preeclampsia_info(user_query)\n",
    "    print(info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029d6dc",
   "metadata": {},
   "source": [
    "# Chatbot interaction- map reference method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preeclampsia_info(query):\n",
    "    \"\"\"Fetch preeclampsia information using Pinecone and GPT-3.5.\"\"\"\n",
    "    pinecone_results = query_pinecone(query)\n",
    "\n",
    "    if not pinecone_results['matches']:\n",
    "        return \"I couldn't find relevant information in the knowledge base.\"\n",
    "\n",
    "    top_chunks = [match['metadata']['content'] for match in pinecone_results['matches']]\n",
    "    top_chunks_content = \"\\n\".join(top_chunks)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a preeclampsia chatbot. Answer the user's question using the following knowledge:\"},\n",
    "            {\"role\": \"system\", \"content\": top_chunks_content},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response.choices[0].message.content\n",
    "\n",
    "\n",
    "  # Combine the LLM outputs from each chunk\n",
    "  combined_response = \"\\n\".join(responses)\n",
    "\n",
    "  return combined_response\n",
    "\n",
    "# Main loop to interact with the user\n",
    "print(\"Hi!!! I am Clamsy. Ask me questions about PRECLAMSIA\")\n",
    "user_query = \"\"\n",
    "while True:\n",
    "  user_query = input(\"\")\n",
    "  if user_query.lower() == \"quit\":\n",
    "    break\n",
    "\n",
    "  info = get_preeclampsia_info(user_query)\n",
    "  print(info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
